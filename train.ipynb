{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pydub\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from utils import get_end_from_start, get_start_end_from_file\n",
    "from data import read_interview, get_features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pydub.AudioSegment.converter = r\"C:/Users/Kinza/anaconda3/envs/pie/Library/bin/ffmpeg.exe\" #CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading videos and computing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALC_FEATS = False #CHANGE THIS TO TRUE IF YOU HAVE NEW VIDEOS\n",
    "\n",
    "if RECALC_FEATS:\n",
    "    # Load videos and calculate feats\n",
    "    video_folder = 'videos/'\n",
    "    df_name = 'data/notes_entretiens_all.xlsx'\n",
    "\n",
    "    filenames = tqdm(os.listdir(video_folder))\n",
    "    df_startend = get_start_end_from_file(df_name)\n",
    "\n",
    "    interviews = []\n",
    "    for f in filenames : \n",
    "        interviews.append(read_interview(video_folder,df_startend,f))\n",
    "\n",
    "    # Tidy feats and save to csv so you don't have to recalculate everything\n",
    "    feats = [item for sublist in interviews for item in sublist]\n",
    "    feats = get_features(feats)\n",
    "    feats.to_csv(\"data/audio_lexic.csv\")\n",
    "\n",
    "else:\n",
    "    # Loading features from CSV\n",
    "    feats = pd.read_csv('data/audio_lexic.csv')\n",
    "    feats = feats.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label dataframe\n",
    "from data import get_scores, merge_scores_feats\n",
    "df_name = 'data/notes_entretiens_all.xlsx'\n",
    "\n",
    "scores = get_scores(df_name)\n",
    "\n",
    "feats, scores = merge_scores_feats(scores,feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing scoring questions\n",
    "voix = ['Q1','Q2','Q4']\n",
    "texte = ['Q5','Q6']\n",
    "visuel = ['Q3','Q7','Q8','Q21']\n",
    "discours = ['Q9','Q10','Q11','Q13','Q17']\n",
    "motivation = ['Q14','Q15']\n",
    "impression = ['Q12','Q16','Q18','Q19','Q20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cormat = pd.concat([feats, scores], axis=1).corr()\n",
    "\n",
    "cormat = cormat.loc[scores.columns,feats.columns]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(cormat, vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, n=200))\n",
    "\n",
    "#visualize linear correlations between scoring questions and calculated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing only abs(correlations) > 0.6\n",
    "cormat[abs(cormat)>0.6].dropna(axis=0,how='all').dropna(axis=1,how='all').round(2).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: how does the mean pause duration relate to vocabulary score\n",
    "\n",
    "plt.scatter(feats['mean_pauses'],scores['Q5'])\n",
    "plt.xlabel('Mean pause duration')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Did the candidate speak with a rich vocabulary and without mistakes ?');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(feats['NOUN'],scores['Q14'])\n",
    "plt.xlabel('Number of nouns')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Did you feel that the candidate was motivated for the role ?');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(feats['nb_lem'],scores['Q4'])\n",
    "plt.xlabel('Number of lems')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Was the candidat convincing/persuasive in delivering ?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudonimisation\n",
    "import hashlib\n",
    "feats_pca = feats.copy()\n",
    "feats_pca['email'] = list(map(lambda s : s.split('_')[0],feats_pca.index.values))\n",
    "feats_pca['question'] = list(map(lambda s : s.split('_')[1],feats_pca.index.values))\n",
    "\n",
    "\n",
    "feats_pca['email'] = list(map(lambda e : hashlib.md5(e.encode()).hexdigest()[:5],feats_pca.email))\n",
    "feats_pca['email'] = feats_pca.apply(lambda s : s.email+'_'+s.question,axis=1)\n",
    "feats_pca = feats_pca.set_index('email').drop(columns=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import pca\n",
    "\n",
    "score = scores.mean(axis=1)\n",
    "\n",
    "pca_m = pca(n_components=2,normalize=True)\n",
    "\n",
    "features_pca = pca_m.fit_transform(feats_pca.iloc[:,:30])\n",
    "\n",
    "pca_m.compute_topfeat().sort_values('loading').groupby('PC').plot.bar('feature','loading')\n",
    "\n",
    "# Scatter first 2 PCs\n",
    "fig, ax = pca_m.scatter(legend=False)\n",
    "\n",
    "# Make biplot with the number of features\n",
    "fig, ax = pca_m.biplot(n_feat=10,legend=False,label=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(features_pca['PC']['PC1'],features_pca['PC']['PC2'],c=score)\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score relevance importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats,scores,test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "voix = ['Q1','Q2','Q4']\n",
    "texte = ['Q5','Q6']\n",
    "visuel = ['Q3','Q7','Q8','Q21']\n",
    "discours = ['Q9','Q10','Q11','Q13','Q17']\n",
    "motivation = ['Q14','Q15']\n",
    "impression = ['Q12','Q16','Q18','Q19','Q20']\n",
    "best_scores = voix+texte+visuel+discours+motivation+impression\n",
    "\n",
    "mse = []\n",
    "model = RandomForestRegressor()\n",
    "for q in best_scores:\n",
    "    y_tr = y_train[q]\n",
    "    y_te = y_test[q]\n",
    "\n",
    "    model.fit(X_train,y_tr)\n",
    "    mse.append(mean_squared_error(model.predict(X_test),y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(c):\n",
    "    if c in voix:\n",
    "        return 'violet'\n",
    "    elif c in texte:\n",
    "        return 'g'\n",
    "    elif c in visuel:\n",
    "        return 'b'\n",
    "    elif c in discours:\n",
    "        return 'orange'\n",
    "    elif c in motivation:\n",
    "        return 'lightblue'\n",
    "    else:\n",
    "        return 'brown'\n",
    "    \n",
    "colors = pd.DataFrame()\n",
    "colors['column'] = ['Q'+str(i+1) for i in range(len(best_scores))]\n",
    "colors['col'] = colors.apply(lambda c: color(c.column),axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(['Q'+str(i+1) for i in range(len(best_scores))],mse, color=colors['col'])\n",
    "plt.title('Errors of models using one score at a time')\n",
    "plt.xlabel('Question')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Getting rid of useless scores\n",
    "voix = ['Q1','Q4']\n",
    "texte = ['Q5','Q6']\n",
    "visuel = []\n",
    "discours = ['Q9','Q10','Q11','Q13','Q17']\n",
    "motivation = ['Q14']\n",
    "impression = ['Q12','Q16','Q18','Q19']\n",
    "best_scores = voix+texte+visuel+discours+motivation+impression\n",
    "\n",
    "score = scores.copy()\n",
    "score['voix'] = score[voix].mean(axis=1)\n",
    "score['texte'] = score[texte].mean(axis=1)\n",
    "score['discours'] = score[discours].mean(axis=1)\n",
    "score['motivation'] = score[motivation].mean(axis=1)\n",
    "score['impression'] = score[impression].mean(axis=1)\n",
    "score['all'] = score[best_scores].mean(axis=1)\n",
    "\n",
    "score = score[['voix','texte','discours','motivation','impression','all']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feats,score,test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=50,max_depth=50)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "metric = mean_squared_error\n",
    "\n",
    "predictions = np.stack(model.predict(X_train))\n",
    "print(\"Training errors\",[round(metric(np.array(y_train)[:,i],predictions[:,i]),2) for i in range(score.shape[1])])\n",
    "\n",
    "predictions = np.stack(model.predict(X_test))\n",
    "print(\"Testing errors\",[round(metric(np.array(y_test)[:,i],predictions[:,i]),2) for i in range(score.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Classification\n",
    "Scores are not floats here but classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feats,score.round(),test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputClassifier(RandomForestClassifier(n_estimators=20,max_depth=50,))\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "metric = accuracy_score\n",
    "\n",
    "predictions = np.stack(model.predict(X_train))\n",
    "print(\"Training errors\",[round(metric(np.array(y_train)[:,i],predictions[:,i]),2) for i in range(score.shape[1])])\n",
    "\n",
    "predictions1 = np.stack(model.predict(X_test))\n",
    "print(\"Testing errors\",[round(metric(np.array(y_test)[:,i],predictions1[:,i]),2) for i in range(score.shape[1])])\n",
    "\n",
    "#Save model\n",
    "import pickle\n",
    "pickle.dump(model, open(\"models/RandomForestClassifier\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputClassifier(SVC(C=10,kernel=\"linear\"))\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "metric =  accuracy_score\n",
    "\n",
    "predictions = np.stack(model.predict(X_train))\n",
    "print(\"Training errors\",[round(metric(np.array(y_train)[:,i],predictions[:,i]),2) for i in range(score.shape[1])])\n",
    "\n",
    "predictions2 = np.stack(model.predict(X_test))\n",
    "print(\"Testing errors\",[round(metric(np.array(y_test)[:,i],predictions2[:,i]),2) for i in range(score.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputClassifier(MLPClassifier(activation=\"relu\",hidden_layer_sizes=1000,learning_rate_init=0.001,\n",
    "                                early_stopping=True,validation_fraction=0.1,alpha=1,batch_size=10))\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "metric =  accuracy_score\n",
    "\n",
    "predictions = np.stack(model.predict(X_train))\n",
    "acc = [round(metric(np.array(y_train)[:,i],predictions[:,i]),2) for i in range(score.shape[1])]\n",
    "\n",
    "print(\"Training errors\",acc)\n",
    "\n",
    "predictions3 = np.stack(model.predict(X_test))\n",
    "acc = [round(metric(np.array(y_test)[:,i],predictions3[:,i]),2) for i in range(score.shape[1])]\n",
    "\n",
    "print(\"Testing errors\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.stack([predictions1,predictions2,predictions3],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.zeros(predictions.shape[:2])\n",
    "for example in range(predictions.shape[0]):\n",
    "    for s in range(predictions.shape[1]):\n",
    "        counts = np.bincount(predictions[example,s,:].astype(int))\n",
    "        best = np.argmax(counts)\n",
    "        final_predictions[example,s] = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [round(metric(np.array(y_test)[:,i],final_predictions[:,i]),2) for i in range(score.shape[1])]\n",
    "\n",
    "print(acc)\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feats,score['all'].round(),test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model = MLPClassifier(activation=\"relu\",hidden_layer_sizes=2000,\n",
    "#        learning_rate_init=0.001,early_stopping=True,validation_fraction=0.1,\n",
    "#        alpha = 0.1, batch_size=10)\n",
    "\n",
    "model = Pipeline([('scaler',scaler),('model',RandomForestClassifier(n_estimators=20,max_depth=50))])\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "metric =  accuracy_score\n",
    "\n",
    "predictions = model.predict(X_train)\n",
    "print(round(metric(np.array(y_train),predictions),2))\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(round(metric(np.array(y_test),predictions),2))\n",
    "\n",
    "pickle.dump(model, open(\"models/RF_single_output\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "importances = model.__getitem__('model').feature_importances_\n",
    "forest_importances = pd.Series(importances, index=feats.columns)\n",
    "std = np.std([tree.feature_importances_ for tree in model.__getitem__('model').estimators_], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explainability\n",
    "from lime import lime_tabular    \n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(feats.values,feature_names=feats.columns, class_names=['1','2','3','4'],\n",
    "    mode='classification')\n",
    "\n",
    "\n",
    "# asking for explanation for LIME model\n",
    "i = np.random.randint(0, feats.shape[0])\n",
    "exp = explainer.explain_instance(feats.iloc[i,:], model.predict_proba, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('explainer', 'wb') as f:\n",
    "    dill.dump(explainer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22481484a5f7e79f314e40293bb4bf1039ec3aa5f1615d995a7f4d567969c466"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
