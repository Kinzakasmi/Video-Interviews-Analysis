{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install numpy  \n",
    "conda install pandas  \n",
    "conda install matplotlib  \n",
    "conda install ffmpeg  \n",
    "pip install pydub  \n",
    "conda install -c conda-forge librosa  \n",
    "pip install SpeechRecognition  \n",
    "pip install -U scikit-learn  \n",
    "conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pydub\n",
    "pydub.AudioSegment.converter = r\"C:/Users/Kinza/anaconda3/envs/pie/Library/bin/ffmpeg.exe\" #CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split on questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pauses(filename) :\n",
    "    audio = pydub.AudioSegment.from_file('videos/'+filename+'.mp4','mp4')\n",
    "    \n",
    "    audio_chunks = pydub.silence.split_on_silence(audio, \n",
    "        # must be silent for at least 1.5 second\n",
    "        min_silence_len=1500,\n",
    "\n",
    "        # consider it silent if quieter than -16 dBFS\n",
    "        silence_thresh=-50\n",
    "    )\n",
    "\n",
    "    print('There are',len(audio_chunks),'chunks')\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        out_file = 'splits/'+filename+\"_chunk{0}.wav\".format(i)\n",
    "        print(\"Exporting\", out_file)\n",
    "        chunk.export(out_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir('videos/')\n",
    "filenames = list(map(lambda s: s.split('.mp4',2)[0],filenames))\n",
    "\n",
    "# list(map(split_pauses,filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"en ce qui concerne une expérience dans un milieu international je voulais vous parler de mon stage en États-Unis à l'université de Vienne donc je suis parti avec mes camarades de la promotion de français université américaine nos de 8h était espagnol et nous étions au travail on aussi avec un étudiant dans le car donc c'était vraiment très international donc c'était sur la production électrique toutes les semaines et surtout pouvoir travailler sous pression quand c'est nécessaire\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "\n",
    "def speech_recognition(filename):\n",
    "    r = sr.Recognizer()\n",
    "    audio = sr.AudioFile('splits/'+filename+'.wav')\n",
    "    with audio as source:\n",
    "        audio_file = r.record(source)\n",
    "    \n",
    "    result = r.recognize_google(audio_file,language = 'fr-FR', show_all=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "result = speech_recognition(filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrenchStemTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.st = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.st.stem(t) for t in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'international',\n",
       " 'milieu',\n",
       " 'pression',\n",
       " 'product',\n",
       " 'promot',\n",
       " 'stag',\n",
       " 'surtout',\n",
       " 'travail']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
    "text_fts = countvect.fit_transform([result])\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}\n",
    "\n",
    "countvect.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22481484a5f7e79f314e40293bb4bf1039ec3aa5f1615d995a7f4d567969c466"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pie': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
